{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "────────────────────────────────────────────────────────────────────────────\n",
    "Event Study – Final Version (Corrected Output Path)\n",
    "────────────────────────────────────────────────────────────────────────────\n",
    "This script is specifically designed to read WIDE-FORMAT event files, where\n",
    "columns represent event types and values are the corresponding dates.\n",
    "\"\"\"\n",
    "\n",
    "# ── Imports ────────────────────────────────────────────────────────────── #\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_1samp, t\n",
    "\n",
    "# ── Tunables & study design ────────────────────────────────────────────── #\n",
    "EST_WIN_DAYS  = 200\n",
    "EST_BUF_DAYS  = 11\n",
    "EVENT_WINDOWS = [1, 5, 10]\n",
    "MAX_EVENT_WINDOW = max(EVENT_WINDOWS) if EVENT_WINDOWS else 10\n",
    "\n",
    "models = {\n",
    "    \"MM_SPY\":         [\"SPY\"],\n",
    "    \"MM_Gold\":        [\"Gold\"],\n",
    "    \"MM_N100\":        [\"Nasdaq100\"],\n",
    "    \"EM_Gold_SPY\":    [\"Gold\", \"SPY\"],\n",
    "    \"EM_Gold_Nasdaq\": [\"Gold\", \"Nasdaq100\"],\n",
    "}\n",
    "\n",
    "# ── Helper functions ───────────────────────────────────────────────────── #\n",
    "def read_csv_robustly(path: Path, engine: str = 'c', sep=','):\n",
    "    \"\"\"\n",
    "    Reads a CSV file by trying a sequence of common encodings,\n",
    "    using the specified parser engine and separator.\n",
    "    \"\"\"\n",
    "    encodings_to_try = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'latin-1']\n",
    "    if engine == 'python': sep = None # Let python engine auto-detect separator\n",
    "    \n",
    "    for enc in encodings_to_try:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=engine, sep=sep)\n",
    "        except (UnicodeDecodeError, UnicodeError, pd.errors.ParserError):\n",
    "            continue\n",
    "    raise ValueError(f\"Failed to read or parse '{path}'. Please check its encoding, structure, and separator.\")\n",
    "\n",
    "def std_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = (df.columns.str.lower().str.replace(\" \", \"\").str.replace(\".\", \"\", regex=False).str.strip())\n",
    "    return df\n",
    "\n",
    "def load_ret(path: Path) -> pd.Series:\n",
    "    \"\"\"Reads structured price data files with encoding fallback.\"\"\"\n",
    "    # The input 'path' is now expected to be a Path object\n",
    "    df = read_csv_robustly(path) # Uses fast 'c' engine by default\n",
    "    df = std_cols(df)\n",
    "    if {\"date\", \"price\"} - set(df.columns): raise ValueError(f\"'{path}': Must contain 'Date' & 'Price'.\")\n",
    "    df[\"date\"]  = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"price\"] = (df[\"price\"].astype(str).str.replace(\",\", \"\").str.strip().pipe(pd.to_numeric, errors=\"coerce\"))\n",
    "    df = (df.dropna(subset=[\"date\", \"price\"]).set_index(\"date\").sort_index())\n",
    "    asset_name = path.stem\n",
    "    return np.log(df[\"price\"]).diff().rename(asset_name)\n",
    "\n",
    "def addc(x: pd.DataFrame) -> pd.DataFrame:\n",
    "    return sm.add_constant(x, has_constant=\"add\")\n",
    "\n",
    "def reg(y: pd.Series, X: pd.DataFrame) -> pd.Series:\n",
    "    return sm.OLS(y, addc(X)).fit().params\n",
    "\n",
    "def car(ar: pd.Series, evt: pd.Timestamp) -> dict[str, float]:\n",
    "    return {f\"CAR(-{k},+{k})\": ar.loc[evt - pd.Timedelta(days=k) : evt + pd.Timedelta(days=k)].sum() for k in EVENT_WINDOWS}\n",
    "\n",
    "# ── 1) Load all data from external folders ─────────────────────────────── #\n",
    "# --- ADJUSTED: Benchmark Data Loading ---\n",
    "print(\"--- Loading Benchmark Data ---\")\n",
    "BENCHMARK_DIR = Path(\"./benchmark\")\n",
    "if not BENCHMARK_DIR.is_dir(): raise FileNotFoundError(f\"Benchmark directory '{BENCHMARK_DIR}' not found.\")\n",
    "\n",
    "bench_files = {\"Gold\": \"Gold.csv\", \"Nasdaq100\": \"Nasdaq100.csv\", \"SPY\": \"SPY.csv\"}\n",
    "# Load each file by combining the benchmark directory path with the filename\n",
    "bench_ret = {name: load_ret(BENCHMARK_DIR / path) for name, path in bench_files.items()}\n",
    "print(f\"  • Loaded: {', '.join(bench_ret.keys())}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Loading Crypto Asset Data ---\")\n",
    "CRYPTO_DATA_DIR = Path(\"./crypto_data\")\n",
    "if not CRYPTO_DATA_DIR.is_dir(): raise FileNotFoundError(f\"Directory '{CRYPTO_DATA_DIR}' not found.\")\n",
    "# Use Path.glob for a more modern approach\n",
    "crypto_files = list(CRYPTO_DATA_DIR.glob(\"*.csv\"))\n",
    "if not crypto_files: raise FileNotFoundError(f\"No CSV files found in '{CRYPTO_DATA_DIR}'.\")\n",
    "asset_ret = {f.stem: load_ret(f) for f in crypto_files}\n",
    "print(f\"  • Found and loaded {len(asset_ret)} assets.\")\n",
    "\n",
    "print(\"\\n--- Loading Wide-Format Event Calendar Data ---\")\n",
    "EVENTS_DIR = Path(\"./events\")\n",
    "train_events_file = EVENTS_DIR / \"training_set.csv\"\n",
    "test_events_file = EVENTS_DIR / \"test_set.csv\"\n",
    "\n",
    "if not EVENTS_DIR.is_dir(): raise FileNotFoundError(f\"Directory '{EVENTS_DIR}' not found.\")\n",
    "if not train_events_file.is_file(): raise FileNotFoundError(f\"Training file '{train_events_file}' not found.\")\n",
    "if not test_events_file.is_file(): raise FileNotFoundError(f\"Test file '{test_events_file}' not found.\")\n",
    "\n",
    "events = {}\n",
    "\n",
    "def load_wide_events(path: Path, suffix: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads WIDE-FORMAT event files.\n",
    "    It iterates through COLUMNS to build the event dictionary.\n",
    "    \"\"\"\n",
    "    local_events = {}\n",
    "    # Force the use of the flexible 'python' engine for these specific files\n",
    "    # to handle ragged columns and prevent ParserError.\n",
    "    df = read_csv_robustly(path, engine='python')\n",
    "    \n",
    "    # Clean column names before processing\n",
    "    df = std_cols(df)\n",
    "    \n",
    "    for group_name in df.columns:\n",
    "        dates = pd.to_datetime(df[group_name].dropna(), errors='coerce').dropna()\n",
    "        local_events[f\"{group_name}{suffix}\"] = pd.DatetimeIndex(dates)\n",
    "    return local_events\n",
    "\n",
    "events.update(load_wide_events(train_events_file, \"_train\"))\n",
    "print(f\"  • Loaded {len(events)} training groups from '{train_events_file.name}'.\")\n",
    "test_events = load_wide_events(test_events_file, \"_test\")\n",
    "print(f\"  • Loaded {len(test_events)} test groups from '{test_events_file.name}'.\")\n",
    "events.update(test_events)\n",
    "print(f\"  • Total unique event groups to process: {len(events)}\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "panel = (pd.concat([*bench_ret.values(), *asset_ret.values()], axis=1).sort_index().ffill())\n",
    "\n",
    "# ── 2) Loop through events → run regressions & compute CAR ─────────────── #\n",
    "event_rows, daily_ar_rows, estimation_data_rows = [], [], []\n",
    "\n",
    "for asset in asset_ret:\n",
    "    merged = panel[[asset, *bench_files.keys()]].dropna()\n",
    "    for grp, dates in events.items():\n",
    "        for evt in dates:\n",
    "            if evt not in merged.index:\n",
    "                print(f\"Warning: Event date {evt.date()} for group '{grp}' not in price data for asset '{asset}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            est_end   = evt - pd.Timedelta(days=EST_BUF_DAYS)\n",
    "            est_start = est_end - pd.Timedelta(days=EST_WIN_DAYS)\n",
    "            est       = merged.loc[est_start : est_end]\n",
    "            if len(est) < 30:\n",
    "                print(f\"Warning: Insufficient data for event {evt.date()} ('{grp}') for asset '{asset}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            row = {}\n",
    "            for mdl, facs in models.items():\n",
    "                params = reg(est[asset], est[facs])\n",
    "                pred = params[\"const\"] + (merged[facs] * params[facs]).sum(axis=1) if len(facs) > 1 else params[\"const\"] + params[facs[0]] * merged[facs[0]]\n",
    "                ar   = merged[asset] - pred\n",
    "                cars = car(ar, evt)\n",
    "\n",
    "                ar_window_series = ar.loc[evt - pd.Timedelta(days=MAX_EVENT_WINDOW) : evt + pd.Timedelta(days=MAX_EVENT_WINDOW)]\n",
    "                \n",
    "                df_ar_temp = ar_window_series.reset_index(name='AR').rename(columns={'date': 'Date'})\n",
    "                df_ar_temp = df_ar_temp.assign(\n",
    "                    RelativeDay = (df_ar_temp['Date'] - evt).dt.days,\n",
    "                    Asset = asset,\n",
    "                    EventGroup = grp,\n",
    "                    EventDate = evt.strftime(\"%Y-%m-%d\"),\n",
    "                    Model = mdl\n",
    "                )\n",
    "                daily_ar_rows.append(df_ar_temp)\n",
    "\n",
    "                df_est_temp = est[[asset, *facs]].copy().reset_index().rename(columns={'date': 'Date', asset: 'AssetReturn'})\n",
    "                df_est_temp = df_est_temp.assign(\n",
    "                    Asset = asset,\n",
    "                    EventGroup = grp,\n",
    "                    EventDate = evt.strftime(\"%Y-%m-%d\"),\n",
    "                    Model = mdl\n",
    "                )\n",
    "                estimation_data_rows.append(df_est_temp)\n",
    "\n",
    "                if mdl.startswith(\"MM\"):\n",
    "                    fac = facs[0]\n",
    "                    row.update({f\"{fac}_α\": params[\"const\"], f\"{fac}_β\": params[fac]})\n",
    "                    for k in EVENT_WINDOWS: row[f\"CAR_MM_{fac}(-{k},+{k})\"] = cars[f\"CAR(-{k},+{k})\"]\n",
    "                elif mdl == \"EM_Gold_SPY\":\n",
    "                    row.update({\"EM_Gold_SPY_α\": params[\"const\"], \"EM_Gold_SPY_β_Gold\": params[\"Gold\"], \"EM_Gold_SPY_β_SPY\": params[\"SPY\"]})\n",
    "                    for k in EVENT_WINDOWS: row[f\"CAR_EM_Gold_SPY(-{k},+{k})\"] = cars[f\"CAR(-{k},+{k})\"]\n",
    "                else: # Assumes EM_Gold_Nasdaq\n",
    "                    row.update({\"EM_Gold_Nasdaq_α\": params[\"const\"], \"EM_Gold_Nasdaq_β_Gold\": params[\"Gold\"], \"EM_Gold_Nasdaq_β_Nasdaq100\": params[\"Nasdaq100\"]})\n",
    "                    for k in EVENT_WINDOWS: row[f\"CAR_EM_Gold_Nasdaq(-{k},+{k})\"] = cars[f\"CAR(-{k},+{k})\"]\n",
    "\n",
    "            idx = (asset, grp, evt.strftime(\"%Y-%m-%d\"))\n",
    "            event_rows.append((idx, row))\n",
    "\n",
    "# ── 3) Convert results to DataFrames & 4) Calculate Group Means ────────── #\n",
    "if not event_rows:\n",
    "    print(\"\\nNo events were processed. Ending script.\")\n",
    "else:\n",
    "    idx_vals, dict_vals = zip(*event_rows)\n",
    "    df_evt_wide = pd.DataFrame(list(dict_vals), index=pd.MultiIndex.from_tuples(idx_vals, names=[\"Asset\", \"EventGroup\", \"EventDate\"]))\n",
    "    col_seq = []\n",
    "    for fac in [\"SPY\", \"Gold\", \"Nasdaq100\"]: col_seq.extend([f\"{fac}_α\", f\"{fac}_β\", *[f\"CAR_MM_{fac}(-{k},+{k})\" for k in EVENT_WINDOWS]])\n",
    "    col_seq.extend([\"EM_Gold_SPY_α\", \"EM_Gold_SPY_β_Gold\", \"EM_Gold_SPY_β_SPY\", *[f\"CAR_EM_Gold_SPY(-{k},+{k})\" for k in EVENT_WINDOWS]])\n",
    "    col_seq.extend([\"EM_Gold_Nasdaq_α\", \"EM_Gold_Nasdaq_β_Gold\", \"EM_Gold_Nasdaq_β_Nasdaq100\", *[f\"CAR_EM_Gold_Nasdaq(-{k},+{k})\" for k in EVENT_WINDOWS]])\n",
    "    df_evt_wide = df_evt_wide.reindex(columns=col_seq)\n",
    "\n",
    "    mean_rows = []\n",
    "    for (asset, grp), sub in df_evt_wide.groupby(level=[\"Asset\", \"EventGroup\"]):\n",
    "        for mdl, facs in models.items():\n",
    "            label = (\"MM_\" + facs[0]) if mdl.startswith(\"MM\") else mdl\n",
    "            for k in EVENT_WINDOWS:\n",
    "                col = (f\"CAR_MM_{facs[0]}(-{k},+{k})\" if mdl.startswith(\"MM\") else f\"CAR_{mdl}(-{k},+{k})\")\n",
    "                if col in sub:\n",
    "                    vals = sub[col].dropna()\n",
    "                    n = len(vals)\n",
    "                    mean, ci_lo, ci_hi, pval = (np.nan,)*4\n",
    "                    if n >= 2:\n",
    "                        mean  = vals.mean()\n",
    "                        se    = vals.std(ddof=1) / np.sqrt(n)\n",
    "                        tcrit = t.ppf(0.975, n-1)\n",
    "                        ci_lo, ci_hi = mean - tcrit*se, mean + tcrit*se\n",
    "                        _, pval = ttest_1samp(vals, 0, nan_policy='omit')\n",
    "                    mean_rows.append({\"Asset\": asset, \"EventGroup\": grp, \"Model\": label, \"Window\": f\"(-{k},+{k})\", \"N\": n, \"MeanCAR\": mean, \"95%CI_low\": ci_lo, \"95%CI_high\": ci_hi, \"p-value\": pval})\n",
    "\n",
    "    df_mean = pd.DataFrame(mean_rows).set_index([\"Asset\", \"EventGroup\", \"Model\", \"Window\"]).sort_index()\n",
    "    df_daily_ar = pd.concat(daily_ar_rows, ignore_index=True) if daily_ar_rows else pd.DataFrame()\n",
    "    df_estimation = pd.concat(estimation_data_rows, ignore_index=True) if estimation_data_rows else pd.DataFrame()\n",
    "\n",
    "    # ── 5) CLI output & Save all data artifacts ────────────────────────────── #\n",
    "    pd.set_option(\"display.max_columns\", None, \"display.width\", 2000, \"display.float_format\", \"{:.6f}\".format)\n",
    "    print(\"\\n==== Event-level wide table (df_evt_wide) [PREVIEW] ====\")\n",
    "    print(df_evt_wide.head())\n",
    "    print(\"\\n==== Asset × EventGroup  MeanCAR ± 95 % CI [PREVIEW] ====\")\n",
    "    print(df_mean.head())\n",
    "\n",
    "    OUTPUT_DIR = Path(\"./outcome\")\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"\\n==== Saving All Output Files to '{OUTPUT_DIR}' folder ====\")\n",
    "\n",
    "    df_evt_wide.to_csv(OUTPUT_DIR / \"event_study_wide_results.csv\")\n",
    "    print(f\" -> Saved event_study_wide_results.csv ({len(df_evt_wide)} rows)\")\n",
    "    df_mean.to_csv(OUTPUT_DIR / \"event_study_mean_results.csv\")\n",
    "    print(f\" -> Saved event_study_mean_results.csv ({len(df_mean)} rows)\")\n",
    "    df_daily_ar.to_csv(OUTPUT_DIR / \"event_study_daily_ar.csv\", index=False)\n",
    "    print(f\" -> Saved event_study_daily_ar.csv ({len(df_daily_ar)} rows)\")\n",
    "    df_estimation.to_csv(OUTPUT_DIR / \"event_study_estimation.csv\", index=False)\n",
    "    print(f\" -> Saved event_study_estimation.csv ({len(df_estimation)} rows)\")\n",
    "    print(\"\\nAnalysis complete. All data files saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
