{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c24e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Batch Regression-Discontinuity (RD) – Price Shocks (Modularized)\n",
    "------------------------------------------------------------------\n",
    "This script runs RDD analysis based on the modular data structure.\n",
    "It builds an Excel workbook in the 'outcome' folder that includes:\n",
    "\n",
    "    • A “Summary” sheet listing τ̂ (treatment effect), p-value, 95 % CI, N, etc.\n",
    "    • An RD plot embedded in each corresponding row.\n",
    "\n",
    "Required for Export:\n",
    "    $ pip install xlsxwriter\n",
    "\"\"\"\n",
    "\n",
    "import io\n",
    "import pathlib\n",
    "import warnings\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1)  Dependency checks\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    HAVE_SM = True\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"statsmodels unavailable ({e}); falling back to NumPy OLS.\")\n",
    "    HAVE_SM = False\n",
    "\n",
    "try:\n",
    "    from scipy import stats as sps\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2)  Global parameters\n",
    "BANDWIDTHS = [10, 20]            # ±n trading-day windows\n",
    "OUTCOME    = \"Price\"             # Column under analysis\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3)  Helper functions\n",
    "def read_csv_robustly(path: pathlib.Path, engine: str = 'c', sep=','):\n",
    "    \"\"\"Reads a CSV file by trying a sequence of common encodings.\"\"\"\n",
    "    encodings_to_try = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'latin-1']\n",
    "    if engine == 'python': sep = None\n",
    "    for enc in encodings_to_try:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=engine, sep=sep)\n",
    "        except (UnicodeDecodeError, UnicodeError, pd.errors.ParserError):\n",
    "            continue\n",
    "    raise ValueError(f\"Failed to read or parse '{path}'.\")\n",
    "\n",
    "def std_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalise column names for consistency.\"\"\"\n",
    "    df.columns = (df.columns.str.lower().str.replace(\" \", \"\").str.replace(\".\", \"\", regex=False).str.strip())\n",
    "    return df\n",
    "\n",
    "def load_price(path: pathlib.Path) -> pd.DataFrame:\n",
    "    \"\"\"Read a CSV, clean the Price column, return a Date-sorted DataFrame.\"\"\"\n",
    "    df = read_csv_robustly(path)\n",
    "    # Standardize column names before checking for them\n",
    "    df = std_cols(df)\n",
    "    \n",
    "    # Check for lowercase 'price' due to std_cols\n",
    "    outcome_lower = OUTCOME.lower()\n",
    "    if \"date\" not in df.columns or outcome_lower not in df.columns:\n",
    "        raise ValueError(f\"File '{path}' must contain 'Date' and '{OUTCOME}' columns.\")\n",
    "        \n",
    "    df = df.rename(columns={outcome_lower: OUTCOME}) # Rename back to original case for compatibility\n",
    "    \n",
    "    df[\"Date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df[OUTCOME] = (df[OUTCOME].astype(str)\n",
    "                           .str.replace(r\"[^0-9\\.\\+\\-eE]\", \"\", regex=True)\n",
    "                           .replace(\"\", np.nan).astype(float))\n",
    "    return df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def ols_numpy(y: np.ndarray, X: np.ndarray):\n",
    "    \"\"\"Lightweight OLS with White SEs; returns beta, p-values, covariance.\"\"\"\n",
    "    n, k = X.shape\n",
    "    beta  = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "    resid = y - X @ beta\n",
    "    sigma2 = (resid @ resid) / (n - k)\n",
    "    cov = sigma2 * np.linalg.inv(X.T @ X)\n",
    "    se = np.sqrt(np.diag(cov))\n",
    "    p = (2 * (1 - sps.t.cdf(np.abs(beta / se), df=n - k)) if HAVE_SCIPY else 2 * (1 - np.exp(-0.5 * (beta / se) ** 2) / np.sqrt(2 * np.pi) / np.abs(beta / se)))\n",
    "    return beta, p, cov\n",
    "\n",
    "\n",
    "def rd_design(df: pd.DataFrame, event_date: pd.Timestamp, bw: int, y_col: str):\n",
    "    \"\"\"Build an RD window and return the window DataFrame and a fitted model.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"D\"], df[\"T\"] = (df[\"Date\"] - event_date).dt.days, (df[\"Date\"] >= event_date).astype(int)\n",
    "    win = df[df[\"D\"].between(-bw, bw)].dropna(subset=[y_col])\n",
    "    X_df = win[[\"T\", \"D\"]].astype(float)\n",
    "    X_df[\"TD\"] = X_df[\"T\"] * X_df[\"D\"]\n",
    "    X, y = np.column_stack([np.ones(len(X_df)), X_df.to_numpy()]), win[y_col].to_numpy(float)\n",
    "    cols = [\"const\", \"T\", \"D\", \"TD\"]\n",
    "    if HAVE_SM:\n",
    "        model = sm.OLS(y, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 3})\n",
    "        model.colnames, model.params, model.pvalues, model.cov = cols, pd.Series(model.params, index=cols), pd.Series(model.pvalues, index=cols), model.cov_params()\n",
    "        return win, model\n",
    "    else:\n",
    "        beta, p, cov = ols_numpy(y, X)\n",
    "        class Result:\n",
    "            params, pvalues, cov, colnames = pd.Series(beta, index=cols), pd.Series(p, index=cols), pd.DataFrame(cov, index=cols, columns=cols), cols\n",
    "            def conf_int(self):\n",
    "                se = np.sqrt(np.diag(self.cov))\n",
    "                return pd.DataFrame(np.column_stack([self.params - 1.96 * se, self.params + 1.96 * se]), index=cols, columns=[\"low\", \"high\"])\n",
    "        return win, Result()\n",
    "\n",
    "\n",
    "def get_ci(model, param: str):\n",
    "    \"\"\"Return (low, high) 95 % CI for *param*, backend-agnostic.\"\"\"\n",
    "    ci = model.conf_int()\n",
    "    return ci.loc[param] if isinstance(ci, pd.DataFrame) else (ci[model.colnames.index(param), 0], ci[model.colnames.index(param), 1])\n",
    "\n",
    "\n",
    "def rd_plot_fig(win: pd.DataFrame, model, evt: pd.Timestamp, bw: int, asset: str, y_col: str):\n",
    "    \"\"\"Create an RD scatter plot and return a matplotlib Figure.\"\"\"\n",
    "    grid = np.arange(-bw, bw + 1)\n",
    "    α, τ, β, γ = (model.params[k] for k in [\"const\", \"T\", \"D\", \"TD\"])\n",
    "    μ_L, μ_R = α + β * grid, α + τ + (β + γ) * grid\n",
    "    cov = model.cov if isinstance(model.cov, np.ndarray) else model.cov.to_numpy()\n",
    "    X_L = np.column_stack([np.ones_like(grid), np.zeros_like(grid), grid, np.zeros_like(grid)])\n",
    "    X_R = np.column_stack([np.ones_like(grid), np.ones_like(grid),  grid, grid])\n",
    "    se_L, se_R = np.sqrt(np.einsum(\"ij,jk,ik->i\", X_L, cov, X_L)), np.sqrt(np.einsum(\"ij,jk,ik->i\", X_R, cov, X_R))\n",
    "    upper_L, lower_L, upper_R, lower_R = μ_L + 1.96 * se_L, μ_L - 1.96 * se_L, μ_R + 1.96 * se_R, μ_R - 1.96 * se_R\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    colors = win[\"D\"].apply(lambda x: \"royalblue\" if x < 0 else \"firebrick\")\n",
    "    ax.scatter(win[\"D\"], win[y_col], s=18, color=colors, alpha=0.7, zorder=2)\n",
    "    ax.plot(grid[grid < 0],  μ_L[grid < 0],  color=\"forestgreen\", lw=2)\n",
    "    ax.plot(grid[grid >= 0], μ_R[grid >= 0], color=\"forestgreen\", lw=2)\n",
    "    ax.fill_between(grid[grid < 0], lower_L[grid < 0], upper_L[grid < 0], color='grey', alpha=0.3)\n",
    "    ax.fill_between(grid[grid >= 0], lower_R[grid >= 0], upper_R[grid >= 0], color='grey', alpha=0.3)\n",
    "    ax.axvline(0, color=\"crimson\", lw=2, ls=\"--\")\n",
    "    ax.set_title(f\"{asset} | {evt.date()} ±{bw}d  τ̂={model.params['T']:.4f}  p={model.pvalues['T']:.3g}\")\n",
    "    ax.set_xlabel(\"Days relative to event\"), ax.set_ylabel(y_col)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4)  Main driver\n",
    "if __name__ == \"__main__\":\n",
    "    # --- ADJUSTED: Dynamic Data Loading ---\n",
    "    print(\"--- Loading Benchmark Data ---\")\n",
    "    # Define the directory where benchmark CSVs are located.\n",
    "    BENCHMARK_DIR = pathlib.Path(\"./benchmark\")\n",
    "    if not BENCHMARK_DIR.is_dir(): raise FileNotFoundError(f\"Benchmark directory '{BENCHMARK_DIR}' not found.\")\n",
    "    \n",
    "    bench_files = [\"Gold.csv\", \"Nasdaq100.csv\", \"SPY.csv\"]\n",
    "    # Load each file from the benchmark directory.\n",
    "    loaded_data = {\n",
    "        pathlib.Path(f).stem: load_price(BENCHMARK_DIR / f) for f in bench_files\n",
    "    }\n",
    "    print(f\"  • Loaded: {', '.join(loaded_data.keys())}\")\n",
    "    \n",
    "    print(\"\\n--- Loading Crypto Asset Data ---\")\n",
    "    CRYPTO_DATA_DIR = pathlib.Path(\"./crypto_data\")\n",
    "    if not CRYPTO_DATA_DIR.is_dir(): raise FileNotFoundError(f\"Directory '{CRYPTO_DATA_DIR}' not found.\")\n",
    "    crypto_files = list(CRYPTO_DATA_DIR.glob(\"*.csv\"))\n",
    "    if not crypto_files: raise FileNotFoundError(f\"No CSV files found in '{CRYPTO_DATA_DIR}'.\")\n",
    "    for f_path in crypto_files:\n",
    "        asset_name = f_path.stem\n",
    "        loaded_data[asset_name] = load_price(f_path)\n",
    "    print(f\"  • Found and loaded {len(crypto_files)} crypto assets.\")\n",
    "\n",
    "    print(\"\\n--- Loading Wide-Format Event Calendar Data ---\")\n",
    "    EVENTS_DIR = pathlib.Path(\"./events\")\n",
    "    train_events_file, test_events_file = EVENTS_DIR / \"training_set.csv\", EVENTS_DIR / \"test_set.csv\"\n",
    "    if not EVENTS_DIR.is_dir(): raise FileNotFoundError(f\"Directory '{EVENTS_DIR}' not found.\")\n",
    "    if not train_events_file.is_file(): raise FileNotFoundError(f\"File '{train_events_file}' not found.\")\n",
    "    if not test_events_file.is_file(): raise FileNotFoundError(f\"File '{test_events_file}' not found.\")\n",
    "    \n",
    "    events = {}\n",
    "    def load_wide_events(path: pathlib.Path, suffix: str) -> dict:\n",
    "        local_events = {}\n",
    "        df = read_csv_robustly(path, engine='python')\n",
    "        df = std_cols(df)\n",
    "        for group_name in df.columns:\n",
    "            local_events[f\"{group_name}{suffix}\"] = pd.to_datetime(df[group_name].dropna(), errors='coerce').dropna()\n",
    "        return local_events\n",
    "\n",
    "    if train_events_file.is_file(): events.update(load_wide_events(train_events_file, \"_train\"))\n",
    "    if test_events_file.is_file(): events.update(load_wide_events(test_events_file, \"_test\"))\n",
    "    print(f\"  • Total unique event groups to process: {len(events)}\")\n",
    "    print(\"----------------------------------------------------\\n\")\n",
    "\n",
    "    # --- Set up output path and Excel writer ---\n",
    "    OUTPUT_DIR = pathlib.Path(\"./outcome\")\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    out_xlsx = OUTPUT_DIR / f\"rd_results_{OUTCOME}.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        writer = pd.ExcelWriter(out_xlsx, engine=\"xlsxwriter\")\n",
    "    except ImportError:\n",
    "        warnings.warn(\"'xlsxwriter' not found. To embed plots in Excel, run: pip install xlsxwriter\")\n",
    "        writer = None # Set writer to None if library is missing\n",
    "\n",
    "    if writer:\n",
    "        workbook = writer.book\n",
    "        ws = workbook.add_worksheet(\"Summary\")\n",
    "        writer.sheets[\"Summary\"] = ws\n",
    "\n",
    "        headers = [\"Asset\", \"Group\", \"Event\", \"Bandwidth\", \"Tau\", \"P_value\", \"CI_Low\", \"CI_High\", \"N_obs\", \"Plot\"]\n",
    "        for c, h in enumerate(headers): ws.write(0, c, h)\n",
    "        current_row = 1\n",
    "\n",
    "    # This list will hold data for a potential fallback CSV export\n",
    "    summary_data_for_csv = []\n",
    "\n",
    "    # --- Main Loop ---\n",
    "    for asset, df in loaded_data.items():\n",
    "        print(f\"\\n=== {asset} ====================================================\")\n",
    "        if df.empty or OUTCOME not in df.columns:\n",
    "            warnings.warn(f\"'{OUTCOME}' column not found or DataFrame is empty for {asset}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        for group, dates in events.items():\n",
    "            for d in dates:\n",
    "                evt = pd.to_datetime(d)\n",
    "                if evt not in df[\"Date\"].values: continue\n",
    "                \n",
    "                for bw in BANDWIDTHS:\n",
    "                    try:\n",
    "                        win, model = rd_design(df, evt, bw, OUTCOME)\n",
    "                        if len(win) < 10:\n",
    "                            warnings.warn(f\"Skipping {asset} on {d.date()} (±{bw}d) due to insufficient data.\")\n",
    "                            continue\n",
    "                        \n",
    "                        ci_lo, ci_hi = get_ci(model, \"T\")\n",
    "                        \n",
    "                        result_dict = {\n",
    "                            \"Asset\": asset, \"Group\": group, \"Event\": d.strftime('%Y-%m-%d'), \"Bandwidth\": bw,\n",
    "                            \"Tau\": float(model.params[\"T\"]), \"P_value\": float(model.pvalues[\"T\"]),\n",
    "                            \"CI_Low\": ci_lo, \"CI_High\": ci_hi, \"N_obs\": len(win)\n",
    "                        }\n",
    "                        summary_data_for_csv.append(result_dict)\n",
    "                        print(f\"{d.date()} {group:<22} ±{bw:>2}d τ̂={result_dict['Tau']:.6f} p={result_dict['P_value']:.4f}\")\n",
    "\n",
    "                        if writer:\n",
    "                            # Write data row to Excel\n",
    "                            ws.write_row(current_row, 0, [result_dict[h] for h in headers if h != 'Plot'])\n",
    "                            # Create plot and insert into Excel\n",
    "                            fig = rd_plot_fig(win, model, evt, bw, asset, OUTCOME)\n",
    "                            buf = io.BytesIO()\n",
    "                            fig.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n",
    "                            buf.seek(0)\n",
    "                            ws.set_row(current_row, 220) # Set row height to fit the plot\n",
    "                            ws.insert_image(current_row, len(headers)-1, f\"img_{asset}_{d.date()}_{bw}\", {\"image_data\": buf, \"x_scale\": 0.8, \"y_scale\": 0.8, 'object_position': 2})\n",
    "                            plt.close(fig)\n",
    "                            current_row += 1\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not process event {d.date()} for {asset} (±{bw}d). Error: {e}\")\n",
    "\n",
    "    # --- Finalize Export ---\n",
    "    if writer:\n",
    "        writer.close()\n",
    "        print(f\"\\nDone! Results and charts saved to {out_xlsx}\")\n",
    "    else:\n",
    "        # Fallback to CSV if xlsxwriter is not available\n",
    "        print(\"\\nExporting summary data to CSV...\")\n",
    "        df_summary = pd.DataFrame(summary_data_for_csv)\n",
    "        csv_path = OUTPUT_DIR / f\"rd_results_{OUTCOME}.csv\"\n",
    "        df_summary.to_csv(csv_path, index=False)\n",
    "        print(f\"Done! Summary data saved to {csv_path}. Plots were not saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
